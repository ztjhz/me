<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.1.0">
<title data-rh="true">TrAVis - Transformer Attention Visualiser | Jing Hua</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://me.tjh.sg/blog/travis"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="TrAVis - Transformer Attention Visualiser | Jing Hua"><meta data-rh="true" name="description" content="How we created an in-browser BERT attention visualiser without a server"><meta data-rh="true" property="og:description" content="How we created an in-browser BERT attention visualiser without a server"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2022-09-28T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/ztjhz,https://github.com/ayaka14732"><meta data-rh="true" property="article:tag" content="nlp,ai,deeplearning,machinelearning,pyodide,d3.js,bert,huggingface,tokeniser,jax,transformer"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://me.tjh.sg/blog/travis"><link data-rh="true" rel="alternate" href="https://me.tjh.sg/blog/travis" hreflang="en"><link data-rh="true" rel="alternate" href="https://me.tjh.sg/blog/travis" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Jing Hua RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Jing Hua Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-EJSWVD6TPY"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-EJSWVD6TPY",{anonymize_ip:!0})</script><link rel="stylesheet" href="/assets/css/styles.1a9f3422.css">
<link rel="preload" href="/assets/js/runtime~main.1c0feb35.js" as="script">
<link rel="preload" href="/assets/js/main.99e1d10e.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):window.matchMedia("(prefers-color-scheme: light)").matches?e("light"):e("dark")}()</script><div id="__docusaurus">
<div role="region" aria-label="theme.common.skipToMainContent"><a href="#" class="skipToContent_fXgn">Skip to main content</a></div><nav class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><b class="navbar__title text--truncate">Jing Hua</b></a><a class="navbar__item navbar__link" href="/about">About</a><a class="navbar__item navbar__link" href="/docs/about">Doc</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/ztjhz" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently dark mode)" aria-label="Switch between dark and light mode (currently dark mode)"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/zilkin">ZilKin - The Future of Zilliqa Smart Contracts</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/bytevid">ByteVid - Deep Learning Hackathon 1st Place</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/blog/travis">TrAVis - Transformer Attention Visualiser</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="title_f1Hy" itemprop="headline">TrAVis - Transformer Attention Visualiser</h1><div class="container_mt6G margin-vert--md"><time datetime="2022-09-28T00:00:00.000Z" itemprop="datePublished">September 28, 2022</time> · <!-- -->3 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/ztjhz" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/ztjhz.png" alt="Jing Hua"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/ztjhz" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jing Hua</span></a></div><small class="avatar__subtitle" itemprop="description">Open source princess</small></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/ayaka14732" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/ayaka14732.png" alt="Ayaka"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/ayaka14732" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Ayaka</span></a></div><small class="avatar__subtitle" itemprop="description">NLP Scientist</small></div></div></div></div></header><div id="post-content" class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="Result" src="/assets/images/result-d51c7e47c5c036d4ae94a4833adc2efe.png" width="1488" height="991" class="img_ev3q"></p><p>How we created an in-browser BERT attention visualiser without a server</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-is-this">What is this?<a class="hash-link" href="#what-is-this" title="Direct link to heading">​</a></h2><p>TrAVis is a Transformer Attention Visualiser. The idea of visualising the attention matrices is inspired by <a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener noreferrer">Neural Machine Translation by Jointly Learning to Align and Translate</a>.</p><p class="centered" style="height:500px"><img loading="lazy" alt="transformer" src="/assets/images/transformer-30b4218690677d30fe3b34e2b964f95b.png" class="img_ev3q"></p><p>The original paper of the Transformer model was named <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer">Attention Is All You Need</a>, demonstrating the centrality of the attention mechanism to <a href="https://huggingface.co/docs/transformers/model_summary" target="_blank" rel="noopener noreferrer">Transformer-based models</a>. These models generate attention matrices during the computation of the attention mechanism, which indicate how the models process the input data, and can therefore be seen as a concrete representation of the mechanism.</p><p>In the <a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener noreferrer">BERT</a> Base Uncased model, for example, there are 12 transformer layers, each layer contains 12 heads, and each head generates one attention matrix. TrAVis is the tool for visualising these attention matrices.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="why-is-it-important">Why is it important?<a class="hash-link" href="#why-is-it-important" title="Direct link to heading">​</a></h2><p>Despite the popularity of Transformer-based models, people often utilise them by just simply running the training scripts, ignoring what is going on inside the model. TrAVis helps us to better understand how Transformer-based models work internally, thus enabling us to better exploit them to solve our problems and, furthermore, giving us inspirations to make improvements to the model architecture.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-did-we-do-it">How did we do it?<a class="hash-link" href="#how-did-we-do-it" title="Direct link to heading">​</a></h2><p>The project consists of 4 parts.</p><p>Firstly, we <a href="https://github.com/ayaka14732/bart-base-jax" target="_blank" rel="noopener noreferrer">implemented</a> the <a href="https://arxiv.org/abs/1910.13461" target="_blank" rel="noopener noreferrer">BART</a> model from scratch using <a href="https://github.com/google/jax" target="_blank" rel="noopener noreferrer">JAX</a>. We chose JAX because it is an amazing deep learning framework that enables us to write clear source code, and it can be easily converted to NumPy, which can be executed in-browser. We chose the #BART model because it is a complete encoder-decoder model, so it can be easily adapted to other models, such as BERT, by simply taking a subset of the source code.</p><p><a href="https://github.com/ayaka14732/bart-base-jax" target="_blank" rel="noopener noreferrer"><img loading="lazy" alt="Bart Base Jax" width="500px" src="/assets/images/bart-base-jax-63d409385bb87c172fe59eb952ff2178.png" class="img_ev3q"></a></p><p>Secondly, we <a href="https://github.com/ztjhz/word-piece-tokenizer" target="_blank" rel="noopener noreferrer">implemented</a> the <a href="https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertTokenizer" target="_blank" rel="noopener noreferrer">HuggingFace BERT Tokeniser</a> in pure Python, as it can be more easily executed in-browser. Moreover, we have optimised the tokenisation algorithm, which is faster than the original HuggingFace implementation.</p><p><a href="https://github.com/ztjhz/word-piece-tokenizer" target="_blank" rel="noopener noreferrer"><img loading="lazy" alt="Word Piece Tokenizer" width="500px" src="/assets/images/word-piece-tokenizer-2993a5e4efc62ffbf6a067dd14dcf2ff.png" class="img_ev3q"></a></p><p>Thirdly, we use <a href="https://pyodide.org" target="_blank" rel="noopener noreferrer">Pyodide</a> to run our Python code in browser. Pyodide supports all Python libraries implemented in pure Python, with <a href="https://pyodide.org/en/stable/usage/packages-in-pyodide.html" target="_blank" rel="noopener noreferrer">additional support</a> for a number of other libraries such as NumPy and SciPy.</p><p>Fourthly, we visualise the attention matrices in our web application using <a href="https://d3js.org/" target="_blank" rel="noopener noreferrer">d3.js</a>.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="result">Result<a class="hash-link" href="#result" title="Direct link to heading">​</a></h2><p>The result is that our Transformer model can run entirely in the browser without the need of a server.</p><p><img loading="lazy" alt="Result" src="/assets/images/result-d51c7e47c5c036d4ae94a4833adc2efe.png" width="1488" height="991" class="img_ev3q"></p><p>When users input sentences into our web application, the loaded model will generate the attention matrices of the sentences, which will then be visualised as a heatmap. Subsequently, users can select which Transformer layer and attention head to visualise, by utilising the range slider.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="source-code">Source code<a class="hash-link" href="#source-code" title="Direct link to heading">​</a></h2><p>The source code of our visualiser is published on GitHub</p><p><a href="https://github.com/ayaka14732/TrAVis" target="_blank" rel="noopener noreferrer"><img loading="lazy" alt="Word Piece Tokenizer" width="500px" src="/assets/images/TrAVis-f631d2f962545c4a4c2bfb263ffdf1c0.png" class="img_ev3q"></a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-team">The team<a class="hash-link" href="#the-team" title="Direct link to heading">​</a></h2><p><a class="container_T1_I" href="https://github.com/ztjhz" title="ztjhz"><img src="https://github.com/ztjhz.png" height="60" width="60"><div>Jing Hua</div></a><a class="container_T1_I" href="https://github.com/ayaka14732" title="ztjhz"><img src="https://github.com/ayaka14732.png" height="60" width="60"><div>Ayaka</div></a></p></div><footer class="row docusaurus-mt-lg blogPostFooterDetailsFull_mRVl"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/nlp">nlp</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/ai">ai</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/deeplearning">deeplearning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/machinelearning">machinelearning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/pyodide">pyodide</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/d-3-js">d3.js</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/bert">bert</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/huggingface">huggingface</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/tokeniser">tokeniser</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/jax">jax</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/transformer">transformer</a></li></ul></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/blog/bytevid"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">ByteVid - Deep Learning Hackathon 1st Place</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#what-is-this" class="table-of-contents__link toc-highlight">What is this?</a></li><li><a href="#why-is-it-important" class="table-of-contents__link toc-highlight">Why is it important?</a></li><li><a href="#how-did-we-do-it" class="table-of-contents__link toc-highlight">How did we do it?</a></li><li><a href="#result" class="table-of-contents__link toc-highlight">Result</a></li><li><a href="#source-code" class="table-of-contents__link toc-highlight">Source code</a></li><li><a href="#the-team" class="table-of-contents__link toc-highlight">The team</a></li></ul></div></div></div></div></div></div>
<script src="/assets/js/runtime~main.1c0feb35.js"></script>
<script src="/assets/js/main.99e1d10e.js"></script>
</body>
</html>