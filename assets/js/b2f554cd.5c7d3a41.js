"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"bytevid","metadata":{"permalink":"/blog/bytevid","source":"@site/blog/2022-10-03-mlda-dlw-post/index.md","title":"ByteVid - Deep Learning Hackathon 1st Place","description":"Say goodbye to long and boring videos! \ud83d\udc4b - MLDA Deep Learning Hackathon 2022 \ud83e\udd47 1st Place","date":"2022-10-03T00:00:00.000Z","formattedDate":"October 3, 2022","tags":[{"label":"hackathon","permalink":"/blog/tags/hackathon"},{"label":"nlp","permalink":"/blog/tags/nlp"},{"label":"cv","permalink":"/blog/tags/cv"},{"label":"deeplearning","permalink":"/blog/tags/deeplearning"},{"label":"ai","permalink":"/blog/tags/ai"},{"label":"whisper","permalink":"/blog/tags/whisper"},{"label":"computervision","permalink":"/blog/tags/computervision"},{"label":"speechrecognition","permalink":"/blog/tags/speechrecognition"},{"label":"objectdetection","permalink":"/blog/tags/objectdetection"},{"label":"machinetranslation","permalink":"/blog/tags/machinetranslation"},{"label":"yolov7","permalink":"/blog/tags/yolov-7"},{"label":"bert","permalink":"/blog/tags/bert"},{"label":"react","permalink":"/blog/tags/react"},{"label":"flask","permalink":"/blog/tags/flask"},{"label":"tailwindcss","permalink":"/blog/tags/tailwindcss"}],"readingTime":7.35,"hasTruncateMarker":true,"authors":[{"name":"Jing Hua","title":"Open source princess","url":"https://github.com/ztjhz","imageURL":"https://github.com/ztjhz.png","key":"nixie"}],"frontMatter":{"slug":"bytevid","title":"ByteVid - Deep Learning Hackathon 1st Place","description":"Say goodbye to long and boring videos! \ud83d\udc4b - MLDA Deep Learning Hackathon 2022 \ud83e\udd47 1st Place","authors":"nixie","tags":["hackathon","nlp","cv","deeplearning","ai","whisper","computervision","speechrecognition","objectdetection","machinetranslation","yolov7","bert","react","flask","tailwindcss"]},"nextItem":{"title":"TrAVis - Transformer Attention Visualiser","permalink":"/blog/travis"}},"content":"![Prize Presentation](./prize-presentation.jpeg)\\n\\n## Say goodbye to long and boring videos! \ud83d\udc4b\\n\\n:::info\\nPowered by the cutting-edge deep learning technologies in 2022, ByteVid transforms long, boring videos into fun byte-sized content. Be it a one hour long lecture, or a 30-minute zoom meeting, ByteVid can transcribe, summarise the content, extract keywords, detect and extract important slides from the video, and translate into other languages.\\n:::\\n\\n\x3c!-- truncate --\x3e\\n\\n> Over the weekend (1 Oct 2022 - 3 Oct 2022), our team comprising of [Jing Hua](https://github.com/ztjhz), [Jing Qiang](https://github.com/xjqx), and [Ayaka](https://github.com/ayaka14732), participated in the MLDA Deep Learning Week Hackathon 2022. We worked tirelessly through exhausting days and sleepless nights, to come up with ByteVid in 48 hours. We went through 2 rounds of pitching and eventually came out on top and finished 1st out of 120 teams \ud83e\udd47. Here is how we did it...\\n\\n## Inspiration\\n\\n<p class=\\"centered\\">\\n    <img alt=\\"smart-nation\\" src={require(\\"./smart-nation.png\\").default} />\\n</p>\\n\\nWhen we first encounter the topic of \u2018AI and Smart Nation\u2019, we were extremely excited as there were tons of areas that we could explore. Mobility, healthcare, media and entertainment, agriculture, social, sustainability, etc. Hours and hours of time were spent on finding something that intrigue us, but we were unsuccessful. How can we balance our skills and aspirations with the problem we want to work on?\\n\\n![Problem](./problem.png)\\n\\nThe idea struck upon us as we went back to our roots as \u2018students\u2019. With Singapore\u2019s effort in promoting Smart Nation, online recorded lectures and meetings are becoming increasingly prevalent. We struggled with online recorded lectures and meetings because it was difficult to understand what the other person was saying. Some spoke in non-native languages. Some spoke with abnormal accents. While some don\u2019t even provide slides for us students to follow the lecture with. We struggled even more given that an average audience attention span is about 7 minutes, and that current Zoom transcription feature is not so accurate. We could not understand the video content properly and with comfort \ud83d\ude25\\n\\nTherefore, we were inspired and motivated to build a project that will help us, and others extract video information efficiently!\\n\\n## The plan\\n\\nWe got to the drawing board and planned for hours on what features we needed and how we could get there. This is what we came up with:\\n\\n![Diagram](./diagram.png)\\n\\n1. We first obtain the video file from the user, either directly or through a YouTube link. If the user supplies a YouTube Link, we utilise [youtube-dl](https://github.com/ytdl-org/youtube-dl) to download the video on to our server.\\n\\n1. We then use ffmepg to compress and speed up the video by 1.6x to optimise the performance of the speech recognition model.\\n\\n1. The compressed video is then passed into [Whisper](https://github.com/openai/whisper), a state-of-the-art (SOTA) speech recognition model, generating a `transcript`.\\n\\n1. Sentences are extracted from the `transcript` using the [BlingFire](https://github.com/microsoft/BlingFire) model to generate an `article`. At the same time, the `transcript` is translated into various languages using [Baidu Translate API](https://api.fanyi.baidu.com/doc/21).\\n\\n1. The article is then passed into the [KBIR-inspec](https://huggingface.co/ml6team/keyphrase-extraction-kbir-inspec) model, which extracts `key phrases`, and the [Bert Extractive Summarizer](https://pypi.org/project/bert-extractive-summarizer/), which generates a `summary`.\\n\\n1. With the `summary` and the `timestamps` from the transcript, we utilise OpenCV to extract `images` from the video of each sentence in the summary.\\n\\n1. With the images, we pass it into our [YOLOv7](https://github.com/ztjhz/yolov7-slides-extraction), a SOTA object detection model that we fine-tuned with manually labelled data, to generate `slide images`.\\n\\n## Seeing it in action\\n\\n### Speech recognition and natural language processing\\n\\nThis is how our speech recognition and natural language processing looks like in action:\\n\\n![Speech and NLP Demo](./speech-nlp-demo.png)\\n\\nUsing 4 different deep learning models and a translation API, the video is transcribed, translated, and transformed into an article, summarised, and extracted into key phrases.\\n\\n### Slides extraction using computer vision\\n\\nThis is how our slides extraction using computer vision looks like in action:\\n\\n![CV Demo](./cv-demo.png)\\n\\nAs you can see, the slides have been extracted from the video.\\n\\n:::info\\nIn fact, our slides detection model is even better than existing solution!\\n\\n![CV Better Demo](./cv-better-demo.png)\\n:::\\n\\nTo achieve such an amazing result, we had to fine-tune the YOLOv7 model for lecture slides detection. To do that, we downloaded a diverse dataset of 200 lecture videos, ranging from computer science lectures, to business seminars, and to zoom meetings. We then manually labelled each and everyone of them using using a [labelling software](https://github.com/Cartucho/OpenLabeling). Subsequently, we trained our [own lecture slides detection model](https://github.com/ztjhz/yolov7-slides-extraction) on our GPU server, and achieved fantastic results!\\n\\n## Bringing ByteVid to the user\\n\\n### Frontend\\n\\nTo make our solution easily accessible to the user, we created a [web application](https://github.com/xJQx/ByteVidFrontend) (built with `React.js` and `Tailwindcss`, and deployed on `GitHub` pages) as well as a [browser extension](https://github.com/ztjhz/ByteVidExtension):\\n\\n![Frontend Demo](./frontend-demo.jpg)\\n\\nThe web application features an intuitive user interface, where users are allowed to choose to submit a YouTube link or upload a video file. The language of the video and the translation language of the transcript can also be customised to their liking.\\n\\n### Backend\\n\\nOur [backend](https://github.com/ayaka14732/ByteVid) utilises a `Flask` server, which is deployed to a GPU machine. Since our GPU machine has no Internet access, we set up a relay server with autossh port forwarding to relay our GPU server port to an Internet-facing VPS. We then utilised Nginx reverse proxy to intergrate our GPU server to our existing web service API. We also utilised Cloudflare for site protection.\\n\\n### The result\\n\\nHere is a demo of our live website:\\n\\nimport ReactPlayer from \'react-player/lazy\';\\n\\n<ReactPlayer url=\\"https://www.youtube.com/watch?v=nhGbsUXVtRA\\" width=\\"100%\\" wrapper=\\"p\\" />\\n\\n### Source code\\n\\n- [ByteVid](https://github.com/ayaka14732/ByteVid)\\n- [ByteVid Front-end](https://github.com/xJQx/ByteVidFrontend)\\n- [ByteVid Extension](https://github.com/ztjhz/ByteVidExtension)\\n- [ByteVid YOLOv7 Slides Extraction](https://github.com/ztjhz/yolov7-slides-extraction)\\n\\n## The benefits\\n\\nByteVid comes with several benefits:\\n\\n1. It works for both business zoom meetings and recorded school lectures, saving time and energy in watching long videos.\\n\\n1. Our translation can overcome language and accent barriers, allowing businesses to enhance their overseas inter-racial collaboration, and students to learn from any lectures of any languages.\\n\\n1. Our extracted summary and slides serve as a template for both business executives and students to build notes upon.\\n\\n## Deep learning models used\\n\\n1. [Whisper](https://github.com/openai/whisper): SOTA speech recognition (Sep 2022)\\n1. [YOLOv7](https://github.com/WongKinYiu/yolov7): SOTA object detection (Jul 2022)\\n1. [KBIR-inspec](https://huggingface.co/ml6team/keyphrase-extraction-kbir-inspec): key phrase extraction (Dec 2021)\\n1. [Bert Extractive Summarizer](https://pypi.org/project/bert-extractive-summarizer/): summarisation (Jun 2019)\\n1. [BlingFire](https://github.com/microsoft/BlingFire): sentence extraction\\n1. [Baidu Translate API](https://api.fanyi.baidu.com/doc/21): translation\\n\\n## Reflection\\n\\n### Accomplishments that we\u2019re proud of\\n\\n- Building and deploying a fully functional AI product in less than 2 days\\n- Our products are a combination of three exciting fields of AI: computer vision, natural language processing and speech processing\\n- We build our own lecture slides dataset and CV model that is better than existing solutions\\n\\n### Challenges\\n\\n- There is no existing solution for lecture slides detection - we manually labelled hundreds of videos and images to train our own lecture slides detection model.\\n- Our GPU server has no Internet access - we set up a relay server with autossh port forwarding.\\n- The ffmpeg commands were complicated - when we finally succeeded in demystifying them, we feel a sense of achievement.\\n- The speech recognition model is relatively slow - we noticed that professors usually speak slowly, so we optimised the performance by speeding up lecture videos by 1.6x before passing them into the speech recognition model.\\n- We used up our Baidu translation API free quota during testing - we paid S$10 to buy extra quota.\\n- The Baidu translation API has a rate limit - we split paragraph into chunks of sentences and request at a moderate speed.\\n- There is no simple method to split paragraphs into sentences (e.g. 3.14 will become two sentences when split by periods) - we utilised the BlingFire model to solve this problem.\\n\\n### What we learned\\n\\n- Deploying deep learning models on cloud server\\n- Speech knowledge for speech recognition and video transcription\\n- NLP knowledge for machine translation, summarisation and keyword extraction\\n- CV knowledge for object detection and lecture slide extraction\\n- Developing a browser extension\\n\\n## What\u2019s next for ByteVid\\n\\n- Auto-navigation to certain timestamps in videos based on keywords\\n- Increase support for other URLs other than YouTube\\n- Implement a Telegram bot\\n- Implement a mobile application\\n\\n## Hackathon Submission\\n\\n[Devpost](https://devpost.com/software/bytevid)\\n\\n## References\\n\\n[1] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \u201cRobust Speech Recognition via Large-Scale Weak Supervision,\u201d 2022.\\n\\n[2] C.-Y. Wang, A. Bochkovskiy, and H.-Y. M. Liao, \u201cYOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors,\u201d arXiv preprint arXiv:2207.02696, 2022.\\n\\n[3] M. Kulkarni, D. Mahata, R. Arora, and R. Bhowmik, \u201cLearning rich representation of keyphrases from text,\u201d arXiv preprint arXiv:2112.08547, 2021.\\n\\n[4] D. Miller, \u201cLeveraging BERT for extractive text summarization on lectures,\u201d arXiv preprint arXiv:1906.04165, 2019.\\n\\nimport GithubUser from \\"@site/src/components/GithubUser\\";\\n\\n## The team\\n\\n<p>\\n    <GithubUser name=\\"Jing Hua\\" url=\\"https://github.com/ztjhz\\" />\\n    <GithubUser name=\\"Ayaka\\" url=\\"https://github.com/ayaka14732\\" />\\n    <GithubUser name=\\"Jing Qiang\\" url=\\"https://github.com/xjqx\\" />\\n</p>"},{"id":"travis","metadata":{"permalink":"/blog/travis","source":"@site/blog/2022-09-28-travis/index.md","title":"TrAVis - Transformer Attention Visualiser","description":"How we created an in-browser BERT attention visualiser without a server","date":"2022-09-28T00:00:00.000Z","formattedDate":"September 28, 2022","tags":[{"label":"nlp","permalink":"/blog/tags/nlp"},{"label":"ai","permalink":"/blog/tags/ai"},{"label":"deeplearning","permalink":"/blog/tags/deeplearning"},{"label":"machinelearning","permalink":"/blog/tags/machinelearning"},{"label":"pyodide","permalink":"/blog/tags/pyodide"},{"label":"d3.js","permalink":"/blog/tags/d-3-js"},{"label":"bert","permalink":"/blog/tags/bert"},{"label":"huggingface","permalink":"/blog/tags/huggingface"},{"label":"tokeniser","permalink":"/blog/tags/tokeniser"},{"label":"jax","permalink":"/blog/tags/jax"},{"label":"transformer","permalink":"/blog/tags/transformer"}],"readingTime":2.465,"hasTruncateMarker":true,"authors":[{"name":"Jing Hua","title":"Open source princess","url":"https://github.com/ztjhz","imageURL":"https://github.com/ztjhz.png","key":"nixie"},{"name":"Ayaka","title":"NLP Scientist","url":"https://github.com/ayaka14732","imageURL":"https://github.com/ayaka14732.png","key":"ayaka"}],"frontMatter":{"slug":"travis","title":"TrAVis - Transformer Attention Visualiser","description":"How we created an in-browser BERT attention visualiser without a server","authors":["nixie","ayaka"],"tags":["nlp","ai","deeplearning","machinelearning","pyodide","d3.js","bert","huggingface","tokeniser","jax","transformer"]},"prevItem":{"title":"ByteVid - Deep Learning Hackathon 1st Place","permalink":"/blog/bytevid"}},"content":"![Result](./result.png)\\n\\nHow we created an in-browser BERT attention visualiser without a server\\n\\n## What is this?\\n\\nTrAVis is a Transformer Attention Visualiser. The idea of visualising the attention matrices is inspired by [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473).\\n\\n\x3c!-- truncate --\x3e\\n\\n<p class=\\"centered\\" style={{height: \\"500px\\"}}>\\n    <img alt=\\"transformer\\"  src={require(\\"./transformer.png\\").default} />\\n</p>\\n\\nThe original paper of the Transformer model was named [Attention Is All You Need](https://arxiv.org/abs/1706.03762), demonstrating the centrality of the attention mechanism to [Transformer-based models](https://huggingface.co/docs/transformers/model_summary). These models generate attention matrices during the computation of the attention mechanism, which indicate how the models process the input data, and can therefore be seen as a concrete representation of the mechanism.\\n\\nIn the [BERT](https://arxiv.org/abs/1810.04805) Base Uncased model, for example, there are 12 transformer layers, each layer contains 12 heads, and each head generates one attention matrix. TrAVis is the tool for visualising these attention matrices.\\n\\n## Why is it important?\\n\\nDespite the popularity of Transformer-based models, people often utilise them by just simply running the training scripts, ignoring what is going on inside the model. TrAVis helps us to better understand how Transformer-based models work internally, thus enabling us to better exploit them to solve our problems and, furthermore, giving us inspirations to make improvements to the model architecture.\\n\\n## How did we do it?\\n\\nThe project consists of 4 parts.\\n\\nFirstly, we [implemented](https://github.com/ayaka14732/bart-base-jax) the [BART](https://arxiv.org/abs/1910.13461) model from scratch using [JAX](https://github.com/google/jax). We chose JAX because it is an amazing deep learning framework that enables us to write clear source code, and it can be easily converted to NumPy, which can be executed in-browser. We chose the #BART model because it is a complete encoder-decoder model, so it can be easily adapted to other models, such as BERT, by simply taking a subset of the source code.\\n\\n[<img alt=\\"Bart Base Jax\\" width=\\"500px\\" src={require(\\"./bart-base-jax.png\\").default} />](https://github.com/ayaka14732/bart-base-jax)\\n\\nSecondly, we [implemented](https://github.com/ztjhz/word-piece-tokenizer) the [HuggingFace BERT Tokeniser](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertTokenizer) in pure Python, as it can be more easily executed in-browser. Moreover, we have optimised the tokenisation algorithm, which is faster than the original HuggingFace implementation.\\n\\n[<img alt=\\"Word Piece Tokenizer\\" width=\\"500px\\" src={require(\\"./word-piece-tokenizer.png\\").default} />](https://github.com/ztjhz/word-piece-tokenizer)\\n\\nThirdly, we use [Pyodide](https://pyodide.org) to run our Python code in browser. Pyodide supports all Python libraries implemented in pure Python, with [additional support](https://pyodide.org/en/stable/usage/packages-in-pyodide.html) for a number of other libraries such as NumPy and SciPy.\\n\\nFourthly, we visualise the attention matrices in our web application using [d3.js](https://d3js.org/).\\n\\n## Result\\n\\nThe result is that our Transformer model can run entirely in the browser without the need of a server.\\n\\n![Result](./result.png)\\n\\nWhen users input sentences into our web application, the loaded model will generate the attention matrices of the sentences, which will then be visualised as a heatmap. Subsequently, users can select which Transformer layer and attention head to visualise, by utilising the range slider.\\n\\n## Source code\\n\\nThe source code of our visualiser is published on GitHub\\n\\n[<img alt=\\"Word Piece Tokenizer\\" width=\\"500px\\" src={require(\\"./TrAVis.png\\").default} />](https://github.com/ayaka14732/TrAVis)\\n\\nimport GithubUser from \\"@site/src/components/GithubUser\\";\\n\\n## The team\\n\\n<p>\\n    <GithubUser name=\\"Jing Hua\\" url=\\"https://github.com/ztjhz\\" />\\n    <GithubUser name=\\"Ayaka\\" url=\\"https://github.com/ayaka14732\\" />\\n</p>"}]}')}}]);